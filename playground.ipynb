{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34062963",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sec_edgar_downloader import Downloader\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "from anyascii import anyascii\n",
    "from bertopic import BERTopic\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "import numpy as np\n",
    "import math\n",
    "from nltk import sent_tokenize, word_tokenize, PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d4b0d8",
   "metadata": {},
   "source": [
    "### Get list of tickers from SEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702cfd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('company_tickers.json') as j:\n",
    "    company_tickers = json.load(j)\n",
    "all_tickers = []\n",
    "for company in company_tickers:\n",
    "    all_tickers.append(company_tickers[company]['ticker'])\n",
    "print(len(all_tickers))\n",
    "all_tickers[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccff7904",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = Downloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ecc795",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = ['AAPL',\"MSFT\",\"V\",'0000102909']\n",
    "ticker = 'AAPL'\n",
    "submissions_folder=f'/sec-edgar-filings/{ticker}/10-K/'\n",
    "ngram_length = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832a663b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_10k_reports_ticker_timeframe(ticker, start_date ,end_date,local=False):\n",
    "    df = pd.DataFrame()\n",
    "    if local:\n",
    "        df_filenames = pd.DataFrame()\n",
    "    dl.get(\"10-K\", ticker, after=start_date, before=end_date)\n",
    "    for folder in os.listdir(os.getcwd() + submissions_folder):\n",
    "        year = folder.split('-')[1]\n",
    "        with open(os.getcwd() + submissions_folder + folder+ '/filing-details.html',encoding='utf-8') as fp:\n",
    "            soup = BeautifulSoup(fp)\n",
    "        text = soup.get_text(strip=True)\n",
    "        text = anyascii(text)\n",
    "        filename = f'data/{year}-{ticker}-report.txt'\n",
    "        if local:\n",
    "            df_filenames = df_filenames.append({\n",
    "                'ticker' : ticker,\n",
    "                'year' :('20' + year),\n",
    "                'filename' : filename\n",
    "            },ignore_index=True)\n",
    "            with open(filename,'w+') as f:\n",
    "                f.write(text)\n",
    "        df = df.append({\n",
    "            'ticker' : ticker,\n",
    "            'year' :('20' + year),\n",
    "            'text' : text\n",
    "        },ignore_index=True)\n",
    "    if local:\n",
    "        return df_filenames\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e607b7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "reports = get_10k_reports_ticker_timeframe(all_tickers,'2015-01-01','2021-01-01', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c546bf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_risk_factors_fragments_from_file(filename, ticker ,year ,local=False):\n",
    "    with open(filename) as f:\n",
    "        text = f.read()\n",
    "    if local:\n",
    "        df = pd.DataFrame()\n",
    "    pos_1a = [m.start() for m in re.finditer('Item 1A', text)]\n",
    "    pos_1b = [m.start() for m in re.finditer('Item 1B', text)]\n",
    "    fragments = {}\n",
    "    index_pos1a = 0\n",
    "    index_pos1b = 0\n",
    "    while index_pos1a < len(pos_1a) and index_pos1b < len(pos_1b):\n",
    "        pos1a = pos_1a[index_pos1a]\n",
    "        pos1b = pos_1b[index_pos1b]\n",
    "        if pos1a * 10 < pos1b:\n",
    "            index_pos1a += 1\n",
    "            continue\n",
    "        if pos1a > pos1b:\n",
    "            index_pos1b += 1\n",
    "            continue\n",
    "        fragments[(pos1a,pos1b)] = text[pos1a:pos1b]\n",
    "        index_pos1a += 1\n",
    "        index_pos1b += 1\n",
    "    \n",
    "    for ifragment in fragments.keys():\n",
    "        if local:\n",
    "            fgr = fragments[ifragment]\n",
    "            df = df.append({\n",
    "                'ticker':ticker,\n",
    "                'year': year,\n",
    "                'start_index':ifragment[0],\n",
    "                'end_index':ifragment[1],\n",
    "                'size':len(fgr),\n",
    "                'text':fgr\n",
    "            },ignore_index=True)\n",
    "            continue\n",
    "        t_filename = filename +'_'+ str(ifragment)+'.txt'\n",
    "        with open(t_filename,'w+') as f:\n",
    "            f.write(fragments[ifragment])\n",
    "    if local:\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f731e553",
   "metadata": {},
   "outputs": [],
   "source": [
    "fragments_df = pd.DataFrame()\n",
    "for (index,row) in reports.iterrows():\n",
    "#     get_risk_factors_fragments_from_file(row['filename'],row['ticker'],row['year'])\n",
    "    df = get_risk_factors_fragments_from_file(row['filename'],row['ticker'],row['year'],local=True)\n",
    "    fragments_df = fragments_df.append(df,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e364c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "fragments_df = pd.DataFrame()\n",
    "for ticker in tickers:\n",
    "    reports = get_10k_reports_ticker_timeframe(ticker,'2015-01-01','2021-01-01', True)\n",
    "    for (index,row) in reports.iterrows():\n",
    "        df = get_risk_factors_fragments_from_file(row['filename'],row['ticker'],row['year'],local=True)\n",
    "        fragments_df = fragments_df.append(df,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bdb2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fragments_df = fragments_df[fragments_df['size'] > 25]\n",
    "fragments_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf6559d",
   "metadata": {},
   "source": [
    "### Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f07419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fragments_df.text = fragments_df.apply(lambda row: re.sub(r\"http\\S+\", \"\", row.text).lower(), 1)\n",
    "# fragments_df.text = fragments_df.apply(lambda row: \" \".join(filter(lambda x:x[0]!=\"@\", row.text.split())), 1)\n",
    "# fragments_df.text = fragments_df.apply(lambda row: \" \".join(re.sub(\"[^a-zA-Z0-9]+\", \" \", row.text).split()), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93142db0",
   "metadata": {},
   "source": [
    "Cleaning of data was obtained from here: https://github.com/etattershall/burst-detection/blob/master/Detecting%20Bursty%20Terms%20in%20Computer%20Science.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8897693",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabets = \"([A-Za-z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov)\"\n",
    "htmltags = '<[^>]+>'\n",
    "htmlspecial = '&#?[xX]?[a-zA-Z0-9]{2,8};'\n",
    "\n",
    "start_delimiter = 'documentstart'\n",
    "sent_delimiter = 'sentenceboundary'\n",
    "end_delimiter = 'documentend'\n",
    "\n",
    "delimiters = [start_delimiter, sent_delimiter, end_delimiter]\n",
    "\n",
    "# Download the lemmatisesr\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "# Create a tokeniser\n",
    "count = CountVectorizer(strip_accents='ascii', min_df=1)\n",
    "tokeniser = count.build_analyzer()\n",
    "\n",
    "def normalise_acronymns(text):\n",
    "    '''\n",
    "    Remove the periods in acronyms. \n",
    "    Adapted from the method found at https://stackoverflow.com/a/40197005 \n",
    "    '''\n",
    "    return re.sub(r'(?<!\\w)([A-Z, a-z])\\.', r'\\1', text)\n",
    "\n",
    "def normalise_decimals(text):\n",
    "    '''\n",
    "    Remove the periods in decimal numbers and replace with POINT\n",
    "    '''\n",
    "    return re.sub(r'([0-9])\\.([0-9])', r'\\1POINT\\2', text)\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    '''\n",
    "    Sentence splitter adapted from https://stackoverflow.com/a/31505798\n",
    "    '''\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    \n",
    "    # my addition\n",
    "    text = re.sub(htmltags, \" \", text)\n",
    "    text = re.sub(htmlspecial, \" \", text)\n",
    "    \n",
    "    if \"FactorsThe\" in text:\n",
    "        text = text.replace(\"FactorsThe\", \"Factors The\")\n",
    "    \n",
    "    if \"Ph.D\" in text: \n",
    "        text = text.replace(\"Ph.D.\",\"PhD\")\n",
    "        \n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1\",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1\\\\2\\\\3\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1\\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1 \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1\",text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1\",text)\n",
    "    \n",
    "    if \"”\" in text: \n",
    "        text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: \n",
    "        text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: \n",
    "        text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: \n",
    "        text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "        \n",
    "    text = text.replace(\".\",\"<stop>\")\n",
    "    text = text.replace(\"?\",\"<stop>\")\n",
    "    text = text.replace(\"!\",\"<stop>\")\n",
    "        \n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    \n",
    "    non_empty = []\n",
    "    for s in sentences: \n",
    "        # we require that there be two alphanumeric characters in a row\n",
    "        if len(re.findall(\"[A-Za-z0-9][A-Za-z0-9]\", s)) > 0:\n",
    "            non_empty.append(s)\n",
    "    return non_empty\n",
    "\n",
    "def pad_sentences(sentences):\n",
    "    '''\n",
    "    Takes a list of sentences and returns a string in which:\n",
    "        - The beginning of the abstract is indicated by DOCUMENTSTART\n",
    "        - The end is indicated by DOCUMENTEND\n",
    "        - Sentence boundaries are indicated by SENTENCEBOUNDARY\n",
    "        \n",
    "    The number of delimiters used is dependent on the ngram length\n",
    "    '''\n",
    "    sent_string = (' '+(sent_delimiter+' ')*(ngram_length-1)).join(sentences)\n",
    "    \n",
    "    return (start_delimiter+' ')*(ngram_length-1) + sent_string + (' '+end_delimiter)*(ngram_length-1)\n",
    "    \n",
    "def cleaning_pipeline(row):\n",
    "    '''\n",
    "    Takes a binary string and returns a list of cleaned sentences, stripped of punctuation and lemmatised\n",
    "    '''\n",
    "\n",
    "    text = normalise_decimals(normalise_acronymns(row['text']))\n",
    "    sentences = split_into_sentences(text)\n",
    "    \n",
    "    # strip out punctuation and make lowercase\n",
    "    clean_sentences = []\n",
    "    for s in sentences:\n",
    "        \n",
    "        # Deal with special cases\n",
    "        s = re.sub(r'[-/]', ' ', s)\n",
    "        \n",
    "        # Remove all other punctuation\n",
    "        s = re.sub(r'[^\\w\\s]','',s)\n",
    "                   \n",
    "        clean_sentences.append(s.lower())\n",
    "        \n",
    "    # pad sentences with delimiters\n",
    "    text = pad_sentences(clean_sentences)\n",
    "    \n",
    "    # Lemmatise word by word\n",
    "    lemmas = []\n",
    "    for word in tokeniser(text):\n",
    "        lemmas.append(wnl.lemmatize(word))\n",
    "    \n",
    "    row['clean_text'] = ' '.join(lemmas)\n",
    "    return row\n",
    "\n",
    "def cleaning_pipeline_sentences(text):\n",
    "    '''\n",
    "    Takes a binary string and returns a list of cleaned sentences, stripped of punctuation and lemmatised\n",
    "    '''\n",
    "\n",
    "    text = normalise_decimals(normalise_acronymns(text))\n",
    "    sentences = split_into_sentences(text)\n",
    "    \n",
    "    # strip out punctuation and make lowercase\n",
    "    clean_sentences = []\n",
    "    for s in sentences:\n",
    "        \n",
    "        # Deal with special cases\n",
    "        s = re.sub(r'[-/]', ' ', s)\n",
    "        \n",
    "        # Remove all other punctuation\n",
    "        s = re.sub(r'[^\\w\\s]','',s)\n",
    "                   \n",
    "        clean_sentences.append(s.lower())\n",
    "        \n",
    "    return clean_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806c9156",
   "metadata": {},
   "outputs": [],
   "source": [
    "fragments_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8ee4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fragments_df['clean_text'] = ''\n",
    "fragments_df = fragments_df.apply(cleaning_pipeline, axis=1)\n",
    "fragments_df['clean_text'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9232cc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "fragments_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7315771",
   "metadata": {},
   "source": [
    "TextSummarization https://www.analyticsvidhya.com/blog/2018/11/introduction-text-summarization-textrank-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c26630",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for s in fragments_df['text']:\n",
    "    sentences.append(cleaning_pipeline_sentences(s))\n",
    "sentences = [y for x in sentences for y in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba15077",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775b142d",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = {}\n",
    "f = open('glove.6B/glove.6B.300d.txt', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    word_embeddings[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb5e52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_vectors = []\n",
    "for i in sentences:\n",
    "    if len(i) != 0:\n",
    "        v = sum([word_embeddings.get(w, np.zeros((300,))) for w in i.split()])/(len(i.split())+0.001)\n",
    "    else:\n",
    "        v = np.zeros((300,))\n",
    "    sentence_vectors.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d44af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_mat = np.zeros([len(sentences), len(sentences)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6364e225",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sentences)):\n",
    "    for j in range(len(sentences)):\n",
    "        if i != j:\n",
    "            sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,300), sentence_vectors[j].reshape(1,300))[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a97a217",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx_graph = nx.from_numpy_array(sim_mat)\n",
    "scores = nx.pagerank(nx_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39687159",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf233f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_sentences_list = []\n",
    "set_ranked_sentences = set()\n",
    "for i in range(100):\n",
    "    ranked_sentences_list.append(ranked_sentences[i][1])\n",
    "    set_ranked_sentences.update([ranked_sentences[i][1]])\n",
    "for sentence in set_ranked_sentences:\n",
    "    print(sentence)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dc6efc",
   "metadata": {},
   "source": [
    "### BERTOPIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72408a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = fragments_df['year'].apply(lambda x: pd.Timestamp(x)).to_list()\n",
    "dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2879aa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = []\n",
    "for (index,row) in fragments_df.iterrows():\n",
    "    title = row['ticker'] +'-'+ row['year'] +'-'+ str(row['start_index']) +'-'+ str(row['end_index'])\n",
    "    titles.append(title)\n",
    "titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f54a992",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fragments_df.summary.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0894dff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model = BERTopic(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35a8775",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics,probs = topic_model.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312c8945",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = topic_model.get_topic_info()\n",
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15473bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_over_time = topic_model.topics_over_time(data, ['COVID','Pandemic','ARM','Lockdown','Supply','Mac','Windows'], dates)\n",
    "topics_over_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f195b99c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
