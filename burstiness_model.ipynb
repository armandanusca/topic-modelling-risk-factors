{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import swifter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from pymongo import MongoClient\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import pickle\n",
    "import csv\n",
    "\n",
    "# Clustering\n",
    "from scipy.spatial.distance import squareform\n",
    "from scipy.cluster import hierarchy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "### Select parameters for cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_length = 3\n",
    "min_yearly_df = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabets = \"([A-Za-z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov)\"\n",
    "htmltags = '<[^>]+>'\n",
    "htmlspecial = '&#?[xX]?[a-zA-Z0-9]{2,8};'\n",
    "\n",
    "start_delimiter = 'documentstart'\n",
    "sent_delimiter = 'sentenceboundary'\n",
    "end_delimiter = 'documentend'\n",
    "\n",
    "delimiters = [start_delimiter, sent_delimiter, end_delimiter]\n",
    "\n",
    "# Download the lemmatisesr\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "# Create a tokeniser\n",
    "count = CountVectorizer(strip_accents='ascii', min_df=1)\n",
    "tokeniser = count.build_analyzer()\n",
    "\n",
    "\n",
    "def normalise_acronymns(text):\n",
    "    '''\n",
    "    Remove the periods in acronyms. \n",
    "    Adapted from the method found at https://stackoverflow.com/a/40197005 \n",
    "    '''\n",
    "    return re.sub(r'(?<!\\w)([A-Z, a-z])\\.', r'\\1', text)\n",
    "\n",
    "\n",
    "def normalise_decimals(text):\n",
    "    '''\n",
    "    Remove the periods in decimal numbers and replace with POINT\n",
    "    '''\n",
    "    return re.sub(r'([0-9])\\.([0-9])', r'\\1POINT\\2', text)\n",
    "\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    '''\n",
    "    Sentence splitter adapted from https://stackoverflow.com/a/31505798\n",
    "    '''\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = re.sub(prefixes, \"\\\\1<prd>\", text)\n",
    "    text = re.sub(websites, \"<prd>\\\\1\", text)\n",
    "\n",
    "    # my addition\n",
    "    text = re.sub(htmltags, \" \", text)\n",
    "    text = re.sub(htmlspecial, \" \", text)\n",
    "\n",
    "    if \"Ph.D\" in text:\n",
    "        text = text.replace(\"Ph.D.\", \"PhD\")\n",
    "\n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \", \" \\\\1\", text)\n",
    "    text = re.sub(acronyms+\" \"+starters, \"\\\\1<stop> \\\\2\", text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\", \"\\\\1\\\\2\\\\3\", text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\", \"\\\\1\\\\2\", text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters, \" \\\\1 \\\\2\", text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\", \" \\\\1\", text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\", \" \\\\1\", text)\n",
    "\n",
    "    if \"”\" in text:\n",
    "        text = text.replace(\".”\", \"”.\")\n",
    "    if \"\\\"\" in text:\n",
    "        text = text.replace(\".\\\"\", \"\\\".\")\n",
    "    if \"!\" in text:\n",
    "        text = text.replace(\"!\\\"\", \"\\\"!\")\n",
    "    if \"?\" in text:\n",
    "        text = text.replace(\"?\\\"\", \"\\\"?\")\n",
    "\n",
    "    text = text.replace(\".\", \"<stop>\")\n",
    "    text = text.replace(\"?\", \"<stop>\")\n",
    "    text = text.replace(\"!\", \"<stop>\")\n",
    "\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "\n",
    "    non_empty = []\n",
    "\n",
    "    for s in sentences:\n",
    "        # we require that there be two alphanumeric characters in a row\n",
    "        if len(re.findall(\"[A-Za-z0-9][A-Za-z0-9]\", s)) > 0:\n",
    "            non_empty.append(s)\n",
    "    return non_empty\n",
    "\n",
    "\n",
    "def pad_sentences(sentences):\n",
    "    '''\n",
    "    Takes a list of sentences and returns a string in which:\n",
    "        - The beginning of the abstract is indicated by DOCUMENTSTART\n",
    "        - The end is indicated by DOCUMENTEND\n",
    "        - Sentence boundaries are indicated by SENTENCEBOUNDARY\n",
    "\n",
    "    The number of delimiters used is dependent on the ngram length\n",
    "    '''\n",
    "    sent_string = (' '+(sent_delimiter+' ')*(ngram_length-1)).join(sentences)\n",
    "\n",
    "    return (start_delimiter+' ')*(ngram_length-1) + sent_string + (' '+end_delimiter)*(ngram_length-1)\n",
    "\n",
    "\n",
    "def get_stopwords():\n",
    "    stop = set(stopwords.words('english'))\n",
    "    stop = set([s.replace(\"'\", \"\") for s in stop])\n",
    "\n",
    "    # Add years to prevent spikes\n",
    "    for year in range(1900, 2020):\n",
    "        stop.add(str(year))\n",
    "\n",
    "    # Add small numbers\n",
    "    for num in range(0, 100):\n",
    "        if len(str(num)) < 2:\n",
    "            stop.add(str(num))\n",
    "            num = '0' + str(num)\n",
    "\n",
    "        stop.add(str(num))\n",
    "\n",
    "    # Add these extra stopwords to the list\n",
    "    # TODO: Look through the corpus and decide which are\n",
    "    # extra stopwords needed for this specific domain\n",
    "    extra = [\n",
    "        'use', 'using', 'uses', 'used', 'based', 'including', 'include', 'approach', 'factors', 'business', 'risk'\n",
    "        'wa', 'ha', 'doe', 'item', '1a', 'factor', '1b', '1aitem', '10-k', '1AITEM', 'could'\n",
    "    ]\n",
    "    for word in extra:\n",
    "        stop.add(word)\n",
    "    return stop\n",
    "\n",
    "\n",
    "def cleaning_pipeline(text):\n",
    "    '''\n",
    "    Takes a binary string and returns a list of cleaned sentences, stripped of punctuation and lemmatised\n",
    "    '''\n",
    "\n",
    "    stopwords = get_stopwords()\n",
    "    text = normalise_decimals(normalise_acronymns(text))\n",
    "    text = ' '.join([word for word in text.split() if word not in stopwords])\n",
    "    sentences = split_into_sentences(text)\n",
    "\n",
    "    # strip out punctuation and make lowercase\n",
    "    clean_sentences = []\n",
    "    for s in sentences:\n",
    "\n",
    "        # Deal with special cases\n",
    "        s = re.sub(r'[-/]', ' ', s)\n",
    "\n",
    "        # Remove all other punctuation\n",
    "        s = re.sub(r'[^\\w\\s]', '', s)\n",
    "\n",
    "        clean_sentences.append(s.lower())\n",
    "\n",
    "    # pad sentences with delimiters\n",
    "    text = pad_sentences(clean_sentences)\n",
    "\n",
    "    # Lemmatise word by word\n",
    "    lemmas = []\n",
    "    for word in tokeniser(text):\n",
    "        lemmas.append(wnl.lemmatize(word))\n",
    "\n",
    "    return ' '.join(lemmas)\n",
    "\n",
    "\n",
    "def cleaning_pipeline_df(row):\n",
    "    row['clean_text'] = cleaning_pipeline(row['text'])\n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Centralise data into a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to MongoDB\n",
    "client = MongoClient('127.0.0.1', 27017)\n",
    "db = client.frtp\n",
    "collection = db.documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_find = collection.find({'ticker':'MSFT'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_find = pd.DataFrame(list(test_find))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_find.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_find['clean_text'] = ''\n",
    "test_find['stop_clean_text'] = ''\n",
    "test_find = test_find.swifter.apply(cleaning_pipeline_df,axis=1)\n",
    "# test_find = test_find.apply(lambda row: df_remove_stopwords(row,'clean_text','stop_clean_text'), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_find['clean_text'].iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_years_df = pd.DataFrame()\n",
    "for year in tqdm(range(10, 22)):\n",
    "    result = collection.find({\"year\": str(year)})\n",
    "    df = pd.DataFrame(list(result))\n",
    "    print(df.shape)\n",
    "    df['clean_text'] = ''\n",
    "    df = df.swifter.apply(cleaning_pipeline_df, axis=1)\n",
    "    df.to_csv(f'data/clean_data/{year}.csv')\n",
    "    all_years_df = all_years_df.append(df)\n",
    "all_years_df.to_csv('all_years_df.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "for year in range(10, 21):\n",
    "    df = pd.read_csv(f'data/clean_data/{year}.csv')\n",
    "    print(df.shape)\n",
    "    # The same as above, applied year by year instead.\n",
    "    t0 = time.time()\n",
    "\n",
    "    vectorizer = CountVectorizer(strip_accents='ascii',\n",
    "                                 ngram_range=(1, ngram_length),\n",
    "                                 stop_words=get_stopwords(),\n",
    "                                 min_df=min_yearly_df)\n",
    "\n",
    "    vector = vectorizer.fit_transform(df.clean_text)\n",
    "\n",
    "    # Save the new words\n",
    "    vocab = vocab.union(vectorizer.vocabulary_.keys())\n",
    "\n",
    "    print(year, len(vocab), time.time()-t0)\n",
    "\n",
    "\n",
    "vocabulary = {}\n",
    "i = 0\n",
    "for v in vocab:\n",
    "    # Remove delimiters\n",
    "    if start_delimiter in v:\n",
    "        pass\n",
    "    elif end_delimiter in v:\n",
    "        pass\n",
    "    elif sent_delimiter in v:\n",
    "        pass\n",
    "    else:\n",
    "        vocabulary[v] = i\n",
    "        i += 1\n",
    "\n",
    "print(len(vocabulary.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = []\n",
    "for year in range(10, 21):\n",
    "    df = pd.read_csv(f'data/clean_data/{year}.csv')\n",
    "\n",
    "    # The same as above, applied year by year instead.\n",
    "    t0 = time.time()\n",
    "\n",
    "    vectorizer = CountVectorizer(strip_accents='ascii',\n",
    "                                 ngram_range=(1, ngram_length),\n",
    "                                 stop_words=get_stopwords(),\n",
    "                                 vocabulary=vocabulary)\n",
    "\n",
    "    vectors.append(vectorizer.fit_transform(df.clean_text))\n",
    "\n",
    "    print(year, time.time()-t0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summed_vectors = []\n",
    "\n",
    "for y in range(len(vectors)):\n",
    "    vector = vectors[y]\n",
    "\n",
    "    # Set all elements that are greater than one to one -- we do not care if a word is used multiple times in\n",
    "    # the same document\n",
    "    vector[vector > 1] = 1\n",
    "\n",
    "    # Sum the vector along columns\n",
    "    summed = np.squeeze(np.asarray(np.sum(vector, axis=0)))\n",
    "\n",
    "    # Normalise by dividing by the number of documents in that year\n",
    "    normalised = summed/vector.shape[0]\n",
    "\n",
    "    # Save the summed vector\n",
    "    summed_vectors.append(normalised)\n",
    "\n",
    "# Stack vectors vertically, so that we have the full history of popularity/time for each term\n",
    "stacked_vectors = np.stack(summed_vectors, axis=1)\n",
    "\n",
    "print(stacked_vectors.shape)\n",
    "\n",
    "stacked_vectors = pd.DataFrame(stacked_vectors.transpose(), columns=list(vocabulary.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalisation = stacked_vectors.sum(axis=1)\n",
    "stacked_vectors = stacked_vectors.divide(normalisation, axis=0)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_ma_length = 8\n",
    "short_ma_length = 4\n",
    "signal_line_ma = 2\n",
    "significance_ma_length = 2\n",
    "\n",
    "significance_threshold = 0.0011\n",
    "years_above_significance = 2\n",
    "testing_period = 2\n",
    "\n",
    "# Detection threshold is set such that the top 500 terms are chosen\n",
    "burstiness_threshold_prediction = 0.003\n",
    "burstiness_threshold_detection = 0.00107"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_macd(dataset):\n",
    "    long_ma = dataset.ewm(span=long_ma_length).mean()\n",
    "    short_ma = dataset.ewm(span=short_ma_length).mean()\n",
    "    significance_ma = dataset.ewm(span=significance_ma_length).mean()\n",
    "    macd = short_ma - long_ma\n",
    "    signal = macd.ewm(span=signal_line_ma).mean()\n",
    "    hist = macd - signal\n",
    "    return long_ma, short_ma, significance_ma, macd, signal, hist\n",
    "\n",
    "def calc_significance(stacked_vectors, significance_threshold, n):\n",
    "    # Must have been above the significance threshold for two consecutive timesteps\n",
    "    a = stacked_vectors>significance_threshold\n",
    "    b = a.rolling(window=n).sum()\n",
    "    return stacked_vectors[stacked_vectors.axes[1][np.where(b.max()>=n)[0]]]\n",
    "    \n",
    "def calc_burstiness(hist, scaling_factor):\n",
    "    print(hist.shape)\n",
    "    print(scaling_factor.shape)\n",
    "    print(hist.iloc[long_ma_length-1:])\n",
    "    return hist.iloc[long_ma_length-1:]/scaling_factor\n",
    "\n",
    "def calc_scaling(significance_ma, method):\n",
    "    if method == \"max\":\n",
    "        scaling = significance_ma.iloc[significance_ma_length-1:].max()\n",
    "    elif method == \"mean\":\n",
    "        scaling = significance_ma.iloc[significance_ma_length-1:].mean()\n",
    "    elif method == \"sqrt\":\n",
    "        scaling = np.sqrt(significance_ma.iloc[significance_ma_length-1:].max())\n",
    "    else:\n",
    "        raise ValueError(\"No valid method provided\")      \n",
    "    return scaling\n",
    "\n",
    "def max_burstiness(burstiness, absolute=False):\n",
    "    if absolute:\n",
    "        b = pd.concat([np.abs(burstiness).max(), burstiness.idxmax()], axis=1)\n",
    "    else:\n",
    "        b = pd.concat([burstiness.max(), burstiness.idxmax()], axis=1)\n",
    "    b.columns = [\"max\", \"location\"]\n",
    "    return b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_vectors = calc_significance(stacked_vectors, significance_threshold, years_above_significance)\n",
    "print(stacked_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_ma, short_ma, significance_ma, macd, signal, hist = calc_macd(stacked_vectors)\n",
    "scaling_factor = calc_scaling(significance_ma, \"sqrt\")\n",
    "burstiness_over_time = calc_burstiness(hist, scaling_factor)\n",
    "burstiness = max_burstiness(burstiness_over_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(burstiness[\"max\"]>0.00107))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bursts = list(burstiness[\"max\"].index[np.where(burstiness[\"max\"]>burstiness_threshold_detection)[0]])\n",
    "print(bursts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorise again, using these terms only\n",
    "vectors = []\n",
    "for year in range(10, 21):\n",
    "    df = pd.read_csv(f'data/clean_data/{year}.csv')\n",
    "    \n",
    "    # The same as above, applied year by year instead.\n",
    "    t0 = time.time()\n",
    "\n",
    "\n",
    "    vectorizer = CountVectorizer(strip_accents='ascii', \n",
    "                                 ngram_range=(1,ngram_length),\n",
    "                                 stop_words=get_stopwords(),\n",
    "                                 vocabulary=bursts)\n",
    "\n",
    "    vector = vectorizer.fit_transform(df.clean_text)\n",
    "    \n",
    "    # If any element is larger than one, set it to one\n",
    "    vector.data = np.where(vector.data>0, 1, 0)\n",
    "    \n",
    "    vectors.append(vector)\n",
    "    \n",
    "    print(year, time.time()-t0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cooccurrence = []\n",
    "for v in vectors:\n",
    "    c = v.T*v\n",
    "    c.setdiag(0)\n",
    "    c = c.todense()\n",
    "    cooccurrence.append(c)\n",
    "    \n",
    "all_cooccurrence = np.sum(cooccurrence, axis=0)\n",
    "\n",
    "# Translate co-occurence into a distance\n",
    "dists = 1- all_cooccurrence/all_cooccurrence.max()\n",
    "\n",
    "# Remove the diagonal (squareform requires diagonals be zero)\n",
    "dists -= np.diag(np.diagonal(dists))\n",
    "\n",
    "# Put the distance matrix into the format required by hierachy.linkage\n",
    "flat_dists = squareform(dists)\n",
    "\n",
    "# Get the linkage matrix\n",
    "linkage_matrix = hierarchy.linkage(flat_dists, \"ward\")\n",
    "\n",
    "assignments = hierarchy.fcluster(linkage_matrix, 80, 'maxclust')\n",
    "print(len(bursts))\n",
    "print(len(set(assignments)))\n",
    "\n",
    "clusters = defaultdict(list)\n",
    "\n",
    "for term, assign in zip(bursts, assignments):\n",
    "    clusters[assign].append(term)\n",
    "    \n",
    "for key in sorted(clusters.keys()):\n",
    "    print(key, ':',  ', '.join(clusters[key]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df = pd.DataFrame({\n",
    "    'cluster_id':pd.Series(dtype=int),\n",
    "    'cluster_terms':pd.Series(dtype=str)\n",
    "})\n",
    "for k,v in clusters.items():\n",
    "    terms = ';'.join(v)\n",
    "    entry ={\n",
    "        'cluster_id':k,\n",
    "        'cluster_terms':terms\n",
    "    }\n",
    "    cluster_df = cluster_df.append(entry,ignore_index=True)\n",
    "cluster_df.to_csv('clusters/clusters.csv')\n",
    "cluster_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manually clean cluster - in a separate CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('clusters/clean_clusters.csv')\n",
    "clusters = [d.split(';') for d in df.cluster_terms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "burstvectors = pickle.load(open('burstvectors_500.p', \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prevalence(cluster):\n",
    "    indices = []\n",
    "    for term in cluster:\n",
    "        indices.append(bursts.index(term))\n",
    "\n",
    "    prevalence = []\n",
    "    for year in range(30):\n",
    "        prevalence.append(100*np.sum(np.sum(burstvectors[year][:,indices], axis=1)>0)/burstvectors[year].shape[0])\n",
    "        \n",
    "    return prevalence\n",
    "\n",
    "\n",
    "yplots = 13\n",
    "xplots = 4\n",
    "fig, axs = plt.subplots(yplots, xplots)\n",
    "plt.subplots_adjust(right=1, hspace=0.9, wspace=0.3)\n",
    "plt.suptitle('Prevalence of selected bursty clusters over time', fontsize=14)\n",
    "fig.subplots_adjust(top=0.95)\n",
    "fig.set_figheight(16)\n",
    "fig.set_figwidth(12)\n",
    "x = np.arange(0,30)\n",
    "\n",
    "prevalences = []\n",
    "for i, cluster in enumerate(clusters):\n",
    "    prevalence = get_prevalence(cluster)\n",
    "    prevalences.append(prevalence)\n",
    "    title = cluster_df.cluster_id[i]\n",
    "    axs[int(np.floor((i/xplots)%yplots)), i%xplots].plot(x, prevalence, color='k', ls='-', label=title)\n",
    "    axs[int(np.floor((i/xplots)%yplots)), i%xplots].grid()\n",
    "    ymax=np.ceil(max(prevalence)*2)/2\n",
    "    if ymax == 0.5 and max(prevalence) <0.25:\n",
    "        ymax=0.25\n",
    "    elif ymax == 2.5:\n",
    "        ymax=3\n",
    "    axs[int(np.floor((i/xplots)%yplots)), i%xplots].set_ylim(0,ymax)\n",
    "    axs[int(np.floor((i/xplots)%yplots)), i%xplots].set_xlim(0,30)\n",
    "    axs[int(np.floor((i/xplots)%yplots)), i%xplots].set_title(title, fontsize=12)\n",
    "    \n",
    "    \n",
    "    if i%yplots != yplots-1:\n",
    "        axs[i%yplots, int(np.floor((i/yplots)%xplots))].set_xticklabels([])\n",
    "    else:\n",
    "        axs[i%yplots, int(np.floor((i/yplots)%xplots))].set_xticklabels([1988, 1998, 2008, 2018])\n",
    "        \n",
    "axs[6,0].set_ylabel('Percentage of documents containing term (%)', fontsize=12)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
