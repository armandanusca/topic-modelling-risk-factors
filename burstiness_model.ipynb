{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pickle\n",
    "import re\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import swifter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from pymongo import MongoClient\n",
    "from scipy.cluster import hierarchy\n",
    "from scipy.spatial.distance import squareform\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "### Select parameters for cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_length = 3\n",
    "min_yearly_df = 3\n",
    "start_year = 7\n",
    "end_year = 22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabets = \"([A-Za-z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov)\"\n",
    "htmltags = '<[^>]+>'\n",
    "htmlspecial = '&#?[xX]?[a-zA-Z0-9]{2,8};'\n",
    "\n",
    "start_delimiter = 'documentstart'\n",
    "sent_delimiter = 'sentenceboundary'\n",
    "end_delimiter = 'documentend'\n",
    "\n",
    "delimiters = [start_delimiter, sent_delimiter, end_delimiter]\n",
    "\n",
    "# Download the lemmatisesr\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "# Create a tokeniser\n",
    "count = CountVectorizer(strip_accents='ascii', min_df=1)\n",
    "tokeniser = count.build_analyzer()\n",
    "\n",
    "\n",
    "def normalise_acronymns(text):\n",
    "    '''\n",
    "    Remove the periods in acronyms. \n",
    "    Adapted from the method found at https://stackoverflow.com/a/40197005 \n",
    "    '''\n",
    "    return re.sub(r'(?<!\\w)([A-Z, a-z])\\.', r'\\1', text)\n",
    "\n",
    "\n",
    "def normalise_decimals(text):\n",
    "    '''\n",
    "    Remove the periods in decimal numbers and replace with POINT\n",
    "    '''\n",
    "    return re.sub(r'([0-9])\\.([0-9])', r'\\1POINT\\2', text)\n",
    "\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    '''\n",
    "    Sentence splitter adapted from https://stackoverflow.com/a/31505798\n",
    "    '''\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = re.sub(prefixes, \"\\\\1<prd>\", text)\n",
    "    text = re.sub(websites, \"<prd>\\\\1\", text)\n",
    "\n",
    "    # my addition\n",
    "    text = re.sub(htmltags, \" \", text)\n",
    "    text = re.sub(htmlspecial, \" \", text)\n",
    "\n",
    "    if \"Ph.D\" in text:\n",
    "        text = text.replace(\"Ph.D.\", \"PhD\")\n",
    "\n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \", \" \\\\1\", text)\n",
    "    text = re.sub(acronyms+\" \"+starters, \"\\\\1<stop> \\\\2\", text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\", \"\\\\1\\\\2\\\\3\", text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\", \"\\\\1\\\\2\", text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters, \" \\\\1 \\\\2\", text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\", \" \\\\1\", text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\", \" \\\\1\", text)\n",
    "\n",
    "    if \"”\" in text:\n",
    "        text = text.replace(\".”\", \"”.\")\n",
    "    if \"\\\"\" in text:\n",
    "        text = text.replace(\".\\\"\", \"\\\".\")\n",
    "    if \"!\" in text:\n",
    "        text = text.replace(\"!\\\"\", \"\\\"!\")\n",
    "    if \"?\" in text:\n",
    "        text = text.replace(\"?\\\"\", \"\\\"?\")\n",
    "\n",
    "    text = text.replace(\".\", \"<stop>\")\n",
    "    text = text.replace(\"?\", \"<stop>\")\n",
    "    text = text.replace(\"!\", \"<stop>\")\n",
    "\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "\n",
    "    non_empty = []\n",
    "\n",
    "    for s in sentences:\n",
    "        # we require that there be two alphanumeric characters in a row\n",
    "        if len(re.findall(\"[A-Za-z0-9][A-Za-z0-9]\", s)) > 0:\n",
    "            non_empty.append(s)\n",
    "    return non_empty\n",
    "\n",
    "\n",
    "def pad_sentences(sentences):\n",
    "    '''\n",
    "    Takes a list of sentences and returns a string in which:\n",
    "        - The beginning of the abstract is indicated by DOCUMENTSTART\n",
    "        - The end is indicated by DOCUMENTEND\n",
    "        - Sentence boundaries are indicated by SENTENCEBOUNDARY\n",
    "\n",
    "    The number of delimiters used is dependent on the ngram length\n",
    "    '''\n",
    "    sent_string = (' '+(sent_delimiter+' ')*(ngram_length-1)).join(sentences)\n",
    "\n",
    "    return (start_delimiter+' ')*(ngram_length-1) + sent_string + (' '+end_delimiter)*(ngram_length-1)\n",
    "\n",
    "\n",
    "def get_stopwords():\n",
    "    stop = set(stopwords.words('english'))\n",
    "    stop = set([s.replace(\"'\", \"\") for s in stop])\n",
    "\n",
    "    # Add years to prevent spikes\n",
    "    for year in range(1900, 2020):\n",
    "        stop.add(str(year))\n",
    "\n",
    "    # Add small numbers\n",
    "    for num in range(0, 100):\n",
    "        if len(str(num)) < 2:\n",
    "            stop.add(str(num))\n",
    "            num = '0' + str(num)\n",
    "\n",
    "        stop.add(str(num))\n",
    "\n",
    "    # Add these extra stopwords to the list\n",
    "    # TODO: Look through the corpus and decide which are\n",
    "    # extra stopwords needed for this specific domain\n",
    "    extra = [\n",
    "        'use', 'using', 'uses', 'used', 'based', 'including', 'include', 'approach', 'factors', 'business', 'risk'\n",
    "        'wa', 'ha', 'doe', 'item', '1a', 'factor', '1b', '1aitem', '10-k', '1AITEM', 'could', 'regarding'\n",
    "    ]\n",
    "    for word in extra:\n",
    "        stop.add(word)\n",
    "    return stop\n",
    "\n",
    "\n",
    "def cleaning_pipeline(text):\n",
    "    '''\n",
    "    Takes a binary string and returns a list of cleaned sentences, stripped of punctuation and lemmatised\n",
    "    '''\n",
    "\n",
    "    stopwords = get_stopwords()\n",
    "    text = normalise_decimals(normalise_acronymns(text))\n",
    "    text = ' '.join([word for word in text.split() if word not in stopwords])\n",
    "    sentences = split_into_sentences(text)\n",
    "\n",
    "    # strip out punctuation and make lowercase\n",
    "    clean_sentences = []\n",
    "    for s in sentences:\n",
    "\n",
    "        # Deal with special cases\n",
    "        s = re.sub(r'[-/]', ' ', s)\n",
    "\n",
    "        # Remove all other punctuation\n",
    "        s = re.sub(r'[^\\w\\s]', '', s)\n",
    "\n",
    "        clean_sentences.append(s.lower())\n",
    "\n",
    "    # pad sentences with delimiters\n",
    "    text = pad_sentences(clean_sentences)\n",
    "\n",
    "    # Lemmatise word by word\n",
    "    lemmas = []\n",
    "    for word in tokeniser(text):\n",
    "        lemmas.append(wnl.lemmatize(word))\n",
    "\n",
    "    return ' '.join(lemmas)\n",
    "\n",
    "\n",
    "def cleaning_pipeline_df(row):\n",
    "    row['clean_text'] = cleaning_pipeline(row['text'])\n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Centralise data into a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to MongoDB\n",
    "client = MongoClient('127.0.0.1', 27017)\n",
    "db = client.frtp\n",
    "collection = db.documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_find = collection.find({'ticker':'MSFT'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_find = pd.DataFrame(list(test_find))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_find.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_find['clean_text'] = ''\n",
    "test_find['stop_clean_text'] = ''\n",
    "test_find = test_find.swifter.apply(cleaning_pipeline_df,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_years_df = pd.DataFrame()\n",
    "for year in tqdm(range(start_year, end_year)):\n",
    "    result = collection.find({\"year\": f\"{year:02d}\"})\n",
    "    df = pd.DataFrame(list(result))\n",
    "    df['clean_text'] = ''\n",
    "    df = df.swifter.apply(cleaning_pipeline_df, axis=1)\n",
    "    df.to_csv(f'data/clean_data/{year}.csv')\n",
    "    all_years_df = all_years_df.append(df)\n",
    "all_years_df.to_csv('all_years_df.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "for year in range(start_year, end_year):\n",
    "    df = pd.read_csv(f'data/clean_data/{year}.csv')\n",
    "    # The same as above, applied year by year instead.\n",
    "    t0 = time.time()\n",
    "\n",
    "    vectorizer = CountVectorizer(strip_accents='ascii',\n",
    "                                 ngram_range=(1, ngram_length),\n",
    "                                 stop_words=get_stopwords(),\n",
    "                                 min_df=min_yearly_df)\n",
    "\n",
    "    vector = vectorizer.fit_transform(df.clean_text)\n",
    "\n",
    "    # Save the new words\n",
    "    vocab = vocab.union(vectorizer.vocabulary_.keys())\n",
    "\n",
    "    print(year, len(vocab), time.time()-t0)\n",
    "\n",
    "\n",
    "vocabulary = {}\n",
    "i = 0\n",
    "for v in vocab:\n",
    "    # Remove delimiters\n",
    "    if start_delimiter in v:\n",
    "        pass\n",
    "    elif end_delimiter in v:\n",
    "        pass\n",
    "    elif sent_delimiter in v:\n",
    "        pass\n",
    "    else:\n",
    "        vocabulary[v] = i\n",
    "        i += 1\n",
    "\n",
    "print(len(vocabulary.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = []\n",
    "for year in range(start_year, end_year):\n",
    "    df = pd.read_csv(f'data/clean_data/{year}.csv')\n",
    "\n",
    "    # The same as above, applied year by year instead.\n",
    "    t0 = time.time()\n",
    "\n",
    "    vectorizer = CountVectorizer(strip_accents='ascii',\n",
    "                                 ngram_range=(1, ngram_length),\n",
    "                                 stop_words=get_stopwords(),\n",
    "                                 vocabulary=vocabulary)\n",
    "\n",
    "    vectors.append(vectorizer.fit_transform(df.clean_text))\n",
    "\n",
    "    print(year, time.time()-t0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summed_vectors = []\n",
    "\n",
    "for y in range(len(vectors)):\n",
    "    vector = vectors[y]\n",
    "\n",
    "    # Set all elements that are greater than one to one -- we do not care if a word is used multiple times in\n",
    "    # the same document\n",
    "    vector[vector > 1] = 1\n",
    "\n",
    "    # Sum the vector along columns\n",
    "    summed = np.squeeze(np.asarray(np.sum(vector, axis=0)))\n",
    "\n",
    "    # Normalise by dividing by the number of documents in that year\n",
    "    normalised = summed/vector.shape[0]\n",
    "\n",
    "    # Save the summed vector\n",
    "    summed_vectors.append(normalised)\n",
    "\n",
    "# Stack vectors vertically, so that we have the full history of popularity/time for each term\n",
    "stacked_vectors = np.stack(summed_vectors, axis=1)\n",
    "\n",
    "print(stacked_vectors.shape)\n",
    "\n",
    "stacked_vectors = pd.DataFrame(stacked_vectors.transpose(), columns=list(vocabulary.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalisation = stacked_vectors.sum(axis=1)\n",
    "stacked_vectors = stacked_vectors.divide(normalisation, axis=0)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_ma_length = 8\n",
    "short_ma_length = 4\n",
    "signal_line_ma = 2\n",
    "significance_ma_length = 2\n",
    "\n",
    "significance_threshold = 0.0005\n",
    "years_above_significance = 2\n",
    "testing_period = 2\n",
    "\n",
    "# Detection threshold is set such that the top 500 terms are chosen\n",
    "burstiness_threshold_prediction = 0.0005\n",
    "burstiness_threshold_detection = 0.00049"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_macd(dataset):\n",
    "    long_ma = dataset.ewm(span=long_ma_length).mean()\n",
    "    short_ma = dataset.ewm(span=short_ma_length).mean()\n",
    "    significance_ma = dataset.ewm(span=significance_ma_length).mean()\n",
    "    macd = short_ma - long_ma\n",
    "    signal = macd.ewm(span=signal_line_ma).mean()\n",
    "    hist = macd - signal\n",
    "    return long_ma, short_ma, significance_ma, macd, signal, hist\n",
    "\n",
    "def calc_significance(stacked_vectors, significance_threshold, n):\n",
    "    # Must have been above the significance threshold for two consecutive timesteps\n",
    "    a = stacked_vectors>significance_threshold\n",
    "    b = a.rolling(window=n).sum()\n",
    "    return stacked_vectors[stacked_vectors.axes[1][np.where(b.max()>=n)[0]]]\n",
    "    \n",
    "def calc_burstiness(hist, scaling_factor):\n",
    "    return hist.iloc[long_ma_length-1:]/scaling_factor\n",
    "\n",
    "def calc_scaling(significance_ma, method):\n",
    "    if method == \"max\":\n",
    "        scaling = significance_ma.iloc[significance_ma_length-1:].max()\n",
    "    elif method == \"mean\":\n",
    "        scaling = significance_ma.iloc[significance_ma_length-1:].mean()\n",
    "    elif method == \"sqrt\":\n",
    "        scaling = np.sqrt(significance_ma.iloc[significance_ma_length-1:].max())\n",
    "    else:\n",
    "        raise ValueError(\"No valid method provided\")      \n",
    "    return scaling\n",
    "\n",
    "def max_burstiness(burstiness, absolute=False):\n",
    "    if absolute:\n",
    "        b = pd.concat([np.abs(burstiness).max(), burstiness.idxmax()], axis=1)\n",
    "    else:\n",
    "        try:\n",
    "            b = pd.concat([burstiness.max(), burstiness.idxmax()], axis=1)\n",
    "        except ValueError:\n",
    "            print(burstiness)\n",
    "            return None\n",
    "    b.columns = [\"max\", \"location\"]\n",
    "    return b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_vectors = calc_significance(stacked_vectors, significance_threshold, years_above_significance)\n",
    "print(stacked_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_ma, short_ma, significance_ma, macd, signal, hist = calc_macd(stacked_vectors)\n",
    "scaling_factor = calc_scaling(significance_ma, \"sqrt\")\n",
    "burstiness_over_time = calc_burstiness(hist, scaling_factor)\n",
    "burstiness = max_burstiness(burstiness_over_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(burstiness[\"max\"]>0.0005))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bursts = list(burstiness[\"max\"].index[np.where(burstiness[\"max\"]>burstiness_threshold_detection)[0]])\n",
    "print(bursts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorise again, using these terms only\n",
    "vectors = []\n",
    "for year in range(start_year, end_year):\n",
    "    df = pd.read_csv(f'data/clean_data/{year}.csv')\n",
    "    \n",
    "    # The same as above, applied year by year instead.\n",
    "    t0 = time.time()\n",
    "\n",
    "\n",
    "    vectorizer = CountVectorizer(strip_accents='ascii', \n",
    "                                 ngram_range=(1,ngram_length),\n",
    "                                 stop_words=get_stopwords(),\n",
    "                                 vocabulary=bursts)\n",
    "\n",
    "    vector = vectorizer.fit_transform(df.clean_text)\n",
    "    \n",
    "    # If any element is larger than one, set it to one\n",
    "    vector.data = np.where(vector.data>0, 1, 0)\n",
    "    \n",
    "    vectors.append(vector)\n",
    "    \n",
    "    print(year, time.time()-t0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cooccurrence = []\n",
    "for v in vectors:\n",
    "    c = v.T*v\n",
    "    c.setdiag(0)\n",
    "    c = c.todense()\n",
    "    cooccurrence.append(c)\n",
    "    \n",
    "all_cooccurrence = np.sum(cooccurrence, axis=0)\n",
    "\n",
    "# Translate co-occurence into a distance\n",
    "dists = 1- all_cooccurrence/all_cooccurrence.max()\n",
    "\n",
    "# Remove the diagonal (squareform requires diagonals be zero)\n",
    "dists -= np.diag(np.diagonal(dists))\n",
    "\n",
    "# Put the distance matrix into the format required by hierachy.linkage\n",
    "flat_dists = squareform(dists)\n",
    "\n",
    "# Get the linkage matrix\n",
    "linkage_matrix = hierarchy.linkage(flat_dists, \"ward\")\n",
    "\n",
    "assignments = hierarchy.fcluster(linkage_matrix, 100, 'maxclust')\n",
    "print(len(bursts))\n",
    "print(len(set(assignments)))\n",
    "\n",
    "clusters = defaultdict(list)\n",
    "\n",
    "for term, assign in zip(bursts, assignments):\n",
    "    clusters[assign].append(term)\n",
    "    \n",
    "for key in sorted(clusters.keys()):\n",
    "    print(key, ':',  ', '.join(clusters[key]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df = pd.DataFrame({\n",
    "    'cluster_id':pd.Series(dtype=int),\n",
    "    'cluster_terms':pd.Series(dtype=str)\n",
    "})\n",
    "for k,v in clusters.items():\n",
    "    terms = ';'.join(v)\n",
    "    entry ={\n",
    "        'cluster_id':k,\n",
    "        'cluster_terms':terms\n",
    "    }\n",
    "    cluster_df = cluster_df.append(entry,ignore_index=True)\n",
    "cluster_df.to_csv('clusters/clusters.csv')\n",
    "cluster_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manually clean cluster - in a separate CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('clusters/clusters.csv')\n",
    "clusters = [d.split(';') for d in df.cluster_terms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(dataset):\n",
    "    '''\n",
    "    Compile the features for the prediction step\n",
    "    '''\n",
    "    long_ma = dataset.ewm(span=long_ma_length).mean()\n",
    "    short_ma = dataset.ewm(span=short_ma_length).mean()\n",
    "    significance_ma = dataset.ewm(span=significance_ma_length).mean()\n",
    "    macd = short_ma - long_ma\n",
    "    signal = macd.ewm(span=signal_line_ma).mean()\n",
    "    hist = macd - signal\n",
    "    \n",
    "    scaling_factor = calc_scaling(significance_ma, \"sqrt\")\n",
    "    burstiness_over_time = calc_burstiness(hist, scaling_factor)\n",
    "    burstiness = max_burstiness(burstiness_over_time)\n",
    "    \n",
    "    \n",
    "    X = long_ma.iloc[long_ma_length:].T\n",
    "    scaled_hist = hist.iloc[long_ma_length:]/scaling_factor\n",
    "    scaled_signal = signal.iloc[long_ma_length:]/scaling_factor\n",
    " \n",
    "    Xtra = pd.concat([significance_ma.iloc[-1], \n",
    "                      dataset.iloc[-1],\n",
    "                        significance_ma.iloc[significance_ma_length:].std()/scaling_factor,\n",
    "                        significance_ma.iloc[significance_ma_length:].max(),\n",
    "                        significance_ma.iloc[significance_ma_length:].min(),\n",
    "                      scaling_factor\n",
    "                        ], axis=1)\n",
    "    X = pd.concat([X,scaled_hist.T,scaled_signal.T,Xtra], axis=1)\n",
    "\n",
    "    X.columns = [str(i) for i in range(6)] + [\"hist\"+str(i) for i in range(6)] + [\"signal\"+str(i) for i in range(6)] + [\n",
    "                        \"significance\",\n",
    "                        \"prevalence\",\n",
    "                        \"scaled std\",\n",
    "                        \"max\",\n",
    "                        \"min\",\n",
    "                        \"scaling\"\n",
    "                    ]\n",
    "\n",
    "\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_range = list(range(2010,2020))\n",
    "development_data = {}\n",
    "for year in range(2015, 2020):\n",
    "    year_idx = year_range.index(year)\n",
    "\n",
    "    print(year)\n",
    "    # Use our three-year method to calc significance\n",
    "    valid_vectors = calc_significance(stacked_vectors[:year_idx+1], significance_threshold, 2)\n",
    "    \n",
    "    # Recalculate the macd things based on this more limited dataset\n",
    "    long_ma, short_ma, significance_ma, macd, signal, hist = calc_macd(valid_vectors)\n",
    "    print(long_ma.shape,short_ma.shape,significance_ma.shape,macd.shape,signal.shape,hist.shape)\n",
    "    \n",
    "    \n",
    "    # Calculate scaling factor\n",
    "    scaling_factor = calc_scaling(significance_ma.iloc[max(long_ma_length, year_idx-8):year_idx+1], \"sqrt\")\n",
    "    print(scaling_factor.shape)\n",
    "    \n",
    "    # Calculate the burstiness\n",
    "    burstiness_over_time = calc_burstiness(hist, scaling_factor)\n",
    "    print(\"Burstiness over time\")\n",
    "    print(burstiness_over_time.shape)\n",
    "    burstiness = max_burstiness(burstiness_over_time)\n",
    "    # print(burstiness.shape)\n",
    "    # Choose terms that are above both thresholds (burstiness, and also most recent year was significant)\n",
    "    print(np.mean(burstiness[\"max\"]))\n",
    "    print(np.std(burstiness[\"max\"]))\n",
    "    print(np.std(significance_ma))\n",
    "    burst_idx = np.where((burstiness[\"max\"]>0.00005)&(significance_ma.iloc[year_idx]>significance_threshold))[0]\n",
    "    \n",
    "    # Find the actual names of these terms\n",
    "    bursts = valid_vectors.keys()[burst_idx]\n",
    "    \n",
    "    # Create a new, much smaller dataset\n",
    "    dataset = stacked_vectors[bursts].iloc[year_idx-5:year_idx+1]\n",
    "    \n",
    "    # Get the scaled y values\n",
    "    if year < 2018:\n",
    "        y = stacked_vectors[bursts].iloc[year_idx+testing_period]\n",
    "\n",
    "\n",
    "    # Select features and store the data\n",
    "    development_data[year] = {}\n",
    "    development_data[year][\"X\"] = feature_selection(dataset)\n",
    "    if year < 2018:\n",
    "        development_data[year][\"y\"]=y-development_data[year][\"X\"]['significance']\n",
    "    print(year, len(bursts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(pd.concat([development_data[year][\"X\"] for year in range(2008,2015)]))\n",
    "y = np.array(pd.concat([development_data[year][\"y\"] for year in range(2008,2015)]))\n",
    "y_thresh = np.zeros_like(y)\n",
    "y_thresh[y>0] = 1\n",
    "\n",
    "# Balance the sample\n",
    "X, y_thresh = balanced_subsample(X, y_thresh,subsample_size=1.0)\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y_thresh, test_size=0.33, random_state=1)\n",
    "\n",
    "max_depths = np.linspace(1, 32, 32, endpoint=True)\n",
    "train_results = []\n",
    "test_results = []\n",
    "for max_depth in max_depths:\n",
    "    rf = RandomForestClassifier(n_estimators=100, max_depth=max_depth)\n",
    "    rf.fit(x_train, y_train)\n",
    "    train_pred = rf.predict(x_train)\n",
    "    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, train_pred)\n",
    "    roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "    train_results.append(roc_auc)\n",
    "    y_pred = rf.predict(x_test)\n",
    "    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n",
    "    roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "    test_results.append(roc_auc)\n",
    "\n",
    "\n",
    "line1, = plt.plot(max_depths, train_results, 'b', label=\"Train AUC\")\n",
    "line2, = plt.plot(max_depths, test_results, 'r', label=\"Test AUC\")\n",
    "plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\n",
    "plt.ylabel('AUC score')\n",
    "plt.xlabel('Tree depth')\n",
    "plt.title('Effect of changing the maximum depth of the random forest on classifier accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(pd.concat([development_data[year][\"X\"] for year in range(2008,2015)]))\n",
    "y = np.array(pd.concat([development_data[year][\"y\"] for year in range(2008,2015)]))\n",
    "y_thresh = np.zeros_like(y)\n",
    "y_thresh[y>0] = 1\n",
    "\n",
    "# Balance the sample\n",
    "X, y_thresh = balanced_subsample(X, y_thresh,subsample_size=1.0)\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y_thresh, test_size=0.33, random_state=3)\n",
    "\n",
    "n_estimators = [1, 2, 4, 8, 16, 32, 64, 100, 150, 200, 300, 500, 800]\n",
    "train_results = []\n",
    "test_results = []\n",
    "for estimator in n_estimators:\n",
    "    rf = RandomForestClassifier(n_estimators=estimator, max_depth=13, n_jobs=-1)\n",
    "    rf.fit(x_train, y_train)\n",
    "    train_pred = rf.predict(x_train)\n",
    "    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, train_pred)\n",
    "    roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "    train_results.append(roc_auc)\n",
    "    y_pred = rf.predict(x_test)\n",
    "    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n",
    "    roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "    test_results.append(roc_auc)\n",
    "from matplotlib.legend_handler import HandlerLine2D\n",
    "line1, = plt.plot(n_estimators, train_results, 'b', label='Train AUC')\n",
    "line2, = plt.plot(n_estimators, test_results, 'r', label='Test AUC')\n",
    "plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\n",
    "plt.ylabel('AUC score')\n",
    "plt.xlabel('n_estimators')\n",
    "plt.title('Effect of changing the number of estimators of the random forest on classifier accuracy')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
