{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from pymongo import MongoClient\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "### Select parameters for cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_length = 3\n",
    "min_yearly_df = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabets = \"([A-Za-z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov)\"\n",
    "htmltags = '<[^>]+>'\n",
    "htmlspecial = '&#?[xX]?[a-zA-Z0-9]{2,8};'\n",
    "\n",
    "start_delimiter = 'documentstart'\n",
    "sent_delimiter = 'sentenceboundary'\n",
    "end_delimiter = 'documentend'\n",
    "\n",
    "delimiters = [start_delimiter, sent_delimiter, end_delimiter]\n",
    "\n",
    "# Download the lemmatisesr\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "# Create a tokeniser\n",
    "count = CountVectorizer(strip_accents='ascii', min_df=1)\n",
    "tokeniser = count.build_analyzer()\n",
    "\n",
    "\n",
    "def normalise_acronymns(text):\n",
    "    '''\n",
    "    Remove the periods in acronyms. \n",
    "    Adapted from the method found at https://stackoverflow.com/a/40197005 \n",
    "    '''\n",
    "    return re.sub(r'(?<!\\w)([A-Z, a-z])\\.', r'\\1', text)\n",
    "\n",
    "\n",
    "def normalise_decimals(text):\n",
    "    '''\n",
    "    Remove the periods in decimal numbers and replace with POINT\n",
    "    '''\n",
    "    return re.sub(r'([0-9])\\.([0-9])', r'\\1POINT\\2', text)\n",
    "\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    '''\n",
    "    Sentence splitter adapted from https://stackoverflow.com/a/31505798\n",
    "    '''\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = re.sub(prefixes, \"\\\\1<prd>\", text)\n",
    "    text = re.sub(websites, \"<prd>\\\\1\", text)\n",
    "\n",
    "    # my addition\n",
    "    text = re.sub(htmltags, \" \", text)\n",
    "    text = re.sub(htmlspecial, \" \", text)\n",
    "\n",
    "    if \"Ph.D\" in text:\n",
    "        text = text.replace(\"Ph.D.\", \"PhD\")\n",
    "\n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \", \" \\\\1\", text)\n",
    "    text = re.sub(acronyms+\" \"+starters, \"\\\\1<stop> \\\\2\", text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\", \"\\\\1\\\\2\\\\3\", text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\", \"\\\\1\\\\2\", text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters, \" \\\\1 \\\\2\", text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\", \" \\\\1\", text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\", \" \\\\1\", text)\n",
    "\n",
    "    if \"”\" in text:\n",
    "        text = text.replace(\".”\", \"”.\")\n",
    "    if \"\\\"\" in text:\n",
    "        text = text.replace(\".\\\"\", \"\\\".\")\n",
    "    if \"!\" in text:\n",
    "        text = text.replace(\"!\\\"\", \"\\\"!\")\n",
    "    if \"?\" in text:\n",
    "        text = text.replace(\"?\\\"\", \"\\\"?\")\n",
    "\n",
    "    text = text.replace(\".\", \"<stop>\")\n",
    "    text = text.replace(\"?\", \"<stop>\")\n",
    "    text = text.replace(\"!\", \"<stop>\")\n",
    "\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "\n",
    "    non_empty = []\n",
    "\n",
    "    for s in sentences:\n",
    "        # we require that there be two alphanumeric characters in a row\n",
    "        if len(re.findall(\"[A-Za-z0-9][A-Za-z0-9]\", s)) > 0:\n",
    "            non_empty.append(s)\n",
    "    return non_empty\n",
    "\n",
    "\n",
    "def pad_sentences(sentences):\n",
    "    '''\n",
    "    Takes a list of sentences and returns a string in which:\n",
    "        - The beginning of the abstract is indicated by DOCUMENTSTART\n",
    "        - The end is indicated by DOCUMENTEND\n",
    "        - Sentence boundaries are indicated by SENTENCEBOUNDARY\n",
    "\n",
    "    The number of delimiters used is dependent on the ngram length\n",
    "    '''\n",
    "    sent_string = (' '+(sent_delimiter+' ')*(ngram_length-1)).join(sentences)\n",
    "\n",
    "    return (start_delimiter+' ')*(ngram_length-1) + sent_string + (' '+end_delimiter)*(ngram_length-1)\n",
    "\n",
    "def get_stopwords():\n",
    "    stop = set(stopwords.words('english'))\n",
    "    stop = set([s.replace(\"'\", \"\") for s in stop])\n",
    "\n",
    "    # Add years to prevent spikes\n",
    "    for year in range(1900, 2020):\n",
    "        stop.add(str(year))\n",
    "\n",
    "    # Add small numbers\n",
    "    for num in range(0, 100):\n",
    "        if len(str(num)) < 2:\n",
    "            stop.add(str(num))\n",
    "            num = '0' + str(num)\n",
    "            \n",
    "        stop.add(str(num))\n",
    "        \n",
    "    # Add these extra stopwords to the list\n",
    "    # TODO: Look through the corpus and decide which are \n",
    "    # extra stopwords needed for this specific domain\n",
    "    extra = [\n",
    "        'use', 'using', 'uses', 'used', 'based', 'including', 'include', 'approach','factors','business','risk'\n",
    "        'wa', 'ha', 'doe', 'item', '1a', 'factor','1b'\n",
    "            ]\n",
    "    for word in extra:\n",
    "        stop.add(word)\n",
    "    return stop\n",
    "\n",
    "def cleaning_pipeline(text):\n",
    "    '''\n",
    "    Takes a binary string and returns a list of cleaned sentences, stripped of punctuation and lemmatised\n",
    "    '''\n",
    "\n",
    "    stopwords = get_stopwords()\n",
    "    text = normalise_decimals(normalise_acronymns(text))\n",
    "    text = ' '.join([word for word in text.split() if word not in stopwords])\n",
    "    sentences = split_into_sentences(text)\n",
    "\n",
    "    # strip out punctuation and make lowercase\n",
    "    clean_sentences = []\n",
    "    for s in sentences:\n",
    "\n",
    "        # Deal with special cases\n",
    "        s = re.sub(r'[-/]', ' ', s)\n",
    "\n",
    "        # Remove all other punctuation\n",
    "        s = re.sub(r'[^\\w\\s]', '', s)\n",
    "\n",
    "        clean_sentences.append(s.lower())\n",
    "\n",
    "    # pad sentences with delimiters\n",
    "    text = pad_sentences(clean_sentences)\n",
    "\n",
    "    # Lemmatise word by word\n",
    "    lemmas = []\n",
    "    for word in tokeniser(text):\n",
    "        lemmas.append(wnl.lemmatize(word))\n",
    "\n",
    "    return ' '.join(lemmas)\n",
    "\n",
    "def cleaning_pipeline_df(row):\n",
    "    row['clean_text'] = cleaning_pipeline(row['text'])\n",
    "    return row\n",
    "\n",
    "def df_remove_stopwords(row,column,clean_column):\n",
    "    stopwords = get_stopwords()\n",
    "    text = str(row[column])\n",
    "    row[clean_column] = ' '.join([word for word in text.split() if word not in stopwords])\n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Centralise data into a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to MongoDB\n",
    "client = MongoClient('127.0.0.1', 27017)\n",
    "db = client.frtp\n",
    "collection = db.documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5383790d0c8347bd8af98019ec46dc95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_years_df = pd.DataFrame()\n",
    "for year in tqdm(range(10, 22)):\n",
    "    result = collection.find({\"year\": str(year)})\n",
    "    df = pd.DataFrame(list(result))\n",
    "    df['clean_text'] = ''\n",
    "    df['stop_clean_text'] = ''\n",
    "    df = df.apply(cleaning_pipeline_df,axis=1)\n",
    "    df = df.apply(lambda row: df_remove_stopwords(row,'clean_text','stop_clean_text'), axis=1)\n",
    "    df.to_csv(f'data/clean_data/{year}.csv')\n",
    "    all_years_df = all_years_df.append(df)\n",
    "all_years_df.to_csv('all_years_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "for year in range(15, 21):\n",
    "    df = pd.read_csv(f'data/clean_csv/{year}.csv')\n",
    "    \n",
    "    # The same as above, applied year by year instead.\n",
    "    t0 = time.time()\n",
    "\n",
    "    vectorizer = CountVectorizer(strip_accents='ascii', \n",
    "                                 ngram_range=(1,ngram_length),\n",
    "                                 stop_words=get_stopwords(),\n",
    "                                 min_df=min_yearly_df)\n",
    "\n",
    "\n",
    "    vector = vectorizer.fit_transform(df.sentences)\n",
    "    \n",
    "    # Save the new words\n",
    "    vocab = vocab.union(vectorizer.vocabulary_.keys())\n",
    "    \n",
    "    print(year, len(vocab), time.time()-t0)\n",
    "\n",
    "    \n",
    "vocabulary = {}\n",
    "i = 0\n",
    "for v in vocab:\n",
    "    # Remove delimiters\n",
    "    if start_delimiter in v:\n",
    "        pass\n",
    "    elif end_delimiter in v:\n",
    "        pass\n",
    "    elif sent_delimiter in v:\n",
    "        pass\n",
    "    else:\n",
    "        vocabulary[v] = i\n",
    "        i += 1\n",
    "        \n",
    "print(len(vocabulary.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = []\n",
    "for year in range(15, 21):\n",
    "    df = pd.read_csv(f'data/clean_csv/{year}.csv')\n",
    "    \n",
    "    # The same as above, applied year by year instead.\n",
    "    t0 = time.time()\n",
    "\n",
    "\n",
    "    vectorizer = CountVectorizer(strip_accents='ascii', \n",
    "                                 ngram_range=(1,ngram_length),\n",
    "                                 stop_words=get_stopwords(),\n",
    "                                 vocabulary=vocabulary)\n",
    "\n",
    "\n",
    "    vectors.append(vectorizer.fit_transform(df.text))\n",
    "    \n",
    "    print(year, time.time()-t0)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
